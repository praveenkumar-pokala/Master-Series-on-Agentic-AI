{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c4ca108",
   "metadata": {},
   "source": [
    "\n",
    "# Agent Telemetry & Dashboard Demo\n",
    "\n",
    "This notebook illustrates how to **instrument a LangGraph agent** and collect:\n",
    "\n",
    "- Node execution times  \n",
    "- Simple event logs  \n",
    "- A basic visualization of where time is spent  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996fb6f2",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942f1a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install -q langgraph langchain-openai langchain matplotlib\n",
    "\n",
    "import os, time\n",
    "os.environ.setdefault(\"OPENAI_API_KEY\", \"sk-REPLACE_ME\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f6345d",
   "metadata": {},
   "source": [
    "\n",
    "## 2. A Simple Two-Node Agent\n",
    "\n",
    "We build:\n",
    "\n",
    "- `analysis_node` — interprets the question  \n",
    "- `answer_node` — generates the final response  \n",
    "\n",
    "We wrap each node with a **telemetry decorator**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f67b8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "telemetry_log = []\n",
    "\n",
    "def record_event(node_name: str, start_time: float, end_time: float):\n",
    "    telemetry_log.append({\n",
    "        \"node\": node_name,\n",
    "        \"start\": start_time,\n",
    "        \"end\": end_time,\n",
    "        \"duration\": end_time - start_time,\n",
    "    })\n",
    "\n",
    "def with_telemetry(node_name, fn):\n",
    "    def wrapper(state: MessagesState):\n",
    "        t0 = time.time()\n",
    "        out = fn(state)\n",
    "        t1 = time.time()\n",
    "        record_event(node_name, t0, t1)\n",
    "        return out\n",
    "    return wrapper\n",
    "\n",
    "def analysis_fn(state: MessagesState):\n",
    "    q = state[\"messages\"][0][\"content\"]\n",
    "    prompt = f\"Classify the user's request type (research, coding, planning, other) and extract key entities.\\n\\nRequest: {q}\"\n",
    "    resp = llm.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    return {\"messages\": state[\"messages\"] + [{\"role\": \"assistant\", \"name\": \"analysis\", \"content\": resp.content}]}\n",
    "\n",
    "def answer_fn(state: MessagesState):\n",
    "    analysis = state[\"messages\"][-1][\"content\"]\n",
    "    q = state[\"messages\"][0][\"content\"]\n",
    "    prompt = (\n",
    "        \"You are a helpful assistant. Using the analysis below, answer the original question clearly.\\n\\n\"\n",
    "        f\"Analysis: {analysis}\\n\\n\"\n",
    "        f\"Question: {q}\"\n",
    "    )\n",
    "    resp = llm.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    return {\"messages\": state[\"messages\"] + [{\"role\": \"assistant\", \"name\": \"answer\", \"content\": resp.content}]}\n",
    "\n",
    "analysis_node = with_telemetry(\"analysis\", analysis_fn)\n",
    "answer_node = with_telemetry(\"answer\", answer_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91eecb8",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Build and Run the Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf11ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"analysis\", analysis_node)\n",
    "builder.add_node(\"answer\", answer_node)\n",
    "\n",
    "builder.add_edge(START, \"analysis\")\n",
    "builder.add_edge(\"analysis\", \"answer\")\n",
    "builder.add_edge(\"answer\", END)\n",
    "\n",
    "agent_graph = builder.compile()\n",
    "\n",
    "query = \"Design an agent that uses MCP tools and RAG to summarize internal documents daily.\"\n",
    "state = {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n",
    "\n",
    "result = agent_graph.invoke(state)\n",
    "\n",
    "print(\"Final answer:\\n\")\n",
    "print(result[\"messages\"][-1][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e4d0d1",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Visualize Telemetry\n",
    "\n",
    "We now inspect the collected `telemetry_log` and plot node durations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c396b0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame(telemetry_log)\n",
    "display(df)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(df[\"node\"], df[\"duration\"])\n",
    "plt.ylabel(\"Duration (s)\")\n",
    "plt.title(\"Node Execution Time\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b2f266",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Takeaways\n",
    "\n",
    "- You can **wrap any node** with telemetry logging without changing its core logic.  \n",
    "- Logs can be streamed to:\n",
    "  - a DB (for production analytics)\n",
    "  - a dashboard (Streamlit / Grafana)\n",
    "  - alerting systems (Slack, PagerDuty)  \n",
    "\n",
    "This pattern is the seed of a full **AgentOps dashboard**.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
