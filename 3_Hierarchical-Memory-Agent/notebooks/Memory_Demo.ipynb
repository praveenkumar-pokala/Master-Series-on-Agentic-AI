{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d748a3b",
   "metadata": {},
   "source": [
    "\n",
    "# Hierarchical Memory Agent Demo\n",
    "\n",
    "This notebook illustrates a **three-layer memory** design for agents:\n",
    "\n",
    "- **Short-term memory** — current conversation  \n",
    "- **Episodic memory** — per-task summaries  \n",
    "- **Long-term memory** — vector store across many sessions  \n",
    "\n",
    "We focus on the *conceptual wiring* and simulate behavior with simple Python structures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf3fb1b",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abc87e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install -q langgraph langchain-openai langchain chromadb\n",
    "\n",
    "import os\n",
    "os.environ.setdefault(\"OPENAI_API_KEY\", \"sk-REPLACE_ME\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43862193",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Simple Memory Manager\n",
    "\n",
    "We simulate:\n",
    "\n",
    "- `short_term`: list of recent messages  \n",
    "- `episodic`: list of per-session summaries  \n",
    "- `long_term`: vector store (Chroma) for semantic recall  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9368aa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "@dataclass\n",
    "class MemoryManager:\n",
    "    short_term: List[Dict[str, Any]] = field(default_factory=list)\n",
    "    episodic: List[str] = field(default_factory=list)\n",
    "    long_term_store: Any = None\n",
    "\n",
    "    def setup_long_term(self, persist_dir=\"memory_store\"):\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "        self.long_term_store = Chroma(collection_name=\"agent_memory\", embedding_function=embeddings, persist_directory=persist_dir)\n",
    "\n",
    "    def add_short(self, msg: Dict[str, Any]):\n",
    "        self.short_term.append(msg)\n",
    "\n",
    "    def summarize_to_episode(self, llm):\n",
    "        text = \"\\n\".join(m[\"content\"] for m in self.short_term if m[\"role\"] == \"user\")\n",
    "        prompt = f\"Summarize the key facts the user mentioned in 3 bullet points:\\n\\n{text}\"\n",
    "        summary = llm.invoke([{\"role\": \"user\", \"content\": prompt}]).content\n",
    "        self.episodic.append(summary)\n",
    "        self.long_term_store.add_texts([summary])\n",
    "        self.short_term.clear()\n",
    "        return summary\n",
    "\n",
    "    def recall(self, query: str, k: int = 3):\n",
    "        if self.long_term_store is None:\n",
    "            return []\n",
    "        docs = self.long_term_store.similarity_search(query, k=k)\n",
    "        return [d.page_content for d in docs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f32cf6e",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Simulate Multi-Session Interaction\n",
    "\n",
    "We create:\n",
    "\n",
    "- A user telling the agent about preferences in session 1  \n",
    "- The agent summarizing to episodic + long-term memory  \n",
    "- In session 2, the user asks the agent what it remembers  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45a3203",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "memory = MemoryManager()\n",
    "memory.setup_long_term()\n",
    "\n",
    "# Session 1: user shares details\n",
    "session1_msgs = [\n",
    "    {\"role\": \"user\", \"content\": \"I work on agentic AI systems using LangGraph and MCP.\"},\n",
    "    {\"role\": \"user\", \"content\": \"I care about reproducible research and good engineering practices.\"},\n",
    "    {\"role\": \"user\", \"content\": \"I often build RAG pipelines for enterprise data.\"},\n",
    "]\n",
    "\n",
    "for m in session1_msgs:\n",
    "    memory.add_short(m)\n",
    "\n",
    "summary1 = memory.summarize_to_episode(llm)\n",
    "print(\"Episodic summary from session 1:\\n\", summary1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861731a7",
   "metadata": {},
   "source": [
    "\n",
    "### Session 2: Ask the Agent What It Remembers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8683dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"What do you remember about my work and interests?\"\n",
    "recalled = memory.recall(\"agentic AI and my work\", k=3)\n",
    "\n",
    "print(\"Recalled from long-term memory:\")\n",
    "for r in recalled:\n",
    "    print(\"-\", r)\n",
    "\n",
    "prompt = (\n",
    "    \"Based on the following long-term memory snippets, answer the user's question: \"\n",
    "    \"'What do you remember about my work and interests?'\\n\\n\"\n",
    "    + \"\\n\".join(recalled)\n",
    ")\n",
    "answer = llm.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "print(\"\\nFinal answer:\\n\", answer.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf72c86",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Takeaways\n",
    "\n",
    "- Hierarchical memory lets agents **remember across sessions** without keeping raw logs forever.  \n",
    "- Episodic summaries compress events; vector stores support semantic recall.  \n",
    "- You can plug this `MemoryManager` into a LangGraph state object and let nodes read/write memory.\n",
    "\n",
    "This is the foundation for **long-lived, personality-consistent agents**.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
