{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acac921d",
   "metadata": {},
   "source": [
    "\n",
    "# ReAct + Reflection Agent Demo (LangGraph)\n",
    "\n",
    "This notebook demonstrates the **core cognition loop** behind agentic AI:\n",
    "\n",
    "> **Thought → Action → Observation → Reflection → Improved Thought**\n",
    "\n",
    "We will:\n",
    "- Build a minimal ReAct-style agent in **LangGraph**\n",
    "- Add a simple **Reflection step** that critiques mistakes\n",
    "- Compare answers *with vs without* reflection on the same query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7e7d77",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Setup\n",
    "\n",
    "We use:\n",
    "\n",
    "- `langgraph` for graph-based orchestration  \n",
    "- `langchain-openai` for LLM calls  \n",
    "- A tiny in-notebook tool (a mock knowledge base)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684363a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install -q langgraph langchain-openai langchain mcp  # mcp not strictly needed here but part of your stack\n",
    "\n",
    "import os\n",
    "os.environ.setdefault(\"OPENAI_API_KEY\", \"sk-REPLACE_ME\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870d8d7a",
   "metadata": {},
   "source": [
    "\n",
    "## 2. A Tiny World & Tools\n",
    "\n",
    "We define a simple **knowledge base** as a Python dict and expose it as a tool.\n",
    "\n",
    "The goal:\n",
    "- Let the agent **think** about *what* to look up\n",
    "- Call the tool\n",
    "- Reflect if it missed anything important\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521a0c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Dict\n",
    "from langchain.tools import tool\n",
    "\n",
    "KB: Dict[str, str] = {\n",
    "    \"MCP\": \"Model Context Protocol is a standard for exposing tools and data sources to LLM-based agents.\",\n",
    "    \"LangGraph\": \"LangGraph lets you model LLM applications as graphs of nodes, edges, and state.\",\n",
    "    \"ReAct\": \"ReAct is a reasoning pattern where the LLM alternates between thoughts and actions.\",\n",
    "}\n",
    "\n",
    "@tool\n",
    "def lookup_concept(name: str) -> str:\n",
    "    \"\"\"Look up a short definition for a concept like MCP, LangGraph, or ReAct.\"\"\"\n",
    "    return KB.get(name, f\"No entry found for {name}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea661dec",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Building a ReAct + Reflect Graph\n",
    "\n",
    "We will create:\n",
    "\n",
    "- `llm_node`: produces thoughts & (optionally) tool calls  \n",
    "- `tool_node`: executes tools (`lookup_concept`)  \n",
    "- `reflect_node`: gets the draft answer and tries to improve it  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1be2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langgraph.graph import StateGraph, START, MessagesState\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "tools = [lookup_concept]\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "def llm_node(state: MessagesState):\n",
    "    bound = llm.bind_tools(tools)\n",
    "    response = bound.invoke(state[\"messages\"])\n",
    "    return {\"messages\": state[\"messages\"] + [response]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41d7bb3",
   "metadata": {},
   "source": [
    "\n",
    "### Reflection Node\n",
    "\n",
    "After the LLM uses tools and generates an answer, we can ask it to:\n",
    "\n",
    "- Review its own answer\n",
    "- Check for missing pieces or errors\n",
    "- Produce an **improved** final response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebfc678",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reflection_node(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    # Take the last assistant message as the draft\n",
    "    draft = messages[-1][\"content\"] if isinstance(messages[-1][\"content\"], str) else str(messages[-1][\"content\"])\n",
    "    prompt = (\n",
    "        \"You are a careful reviewer. The following is your OWN draft answer. \"\n",
    "        \"Identify mistakes, missing key ideas, and then produce a refined final answer.\\n\\n\"\n",
    "        f\"Draft answer:\\n{draft}\"\n",
    "    )\n",
    "    review = llm.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    return {\"messages\": messages + [review]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097150a2",
   "metadata": {},
   "source": [
    "\n",
    "### Assemble the Graph\n",
    "\n",
    "We want:\n",
    "\n",
    "```text\n",
    "START → llm → (tools?) → llm → reflection → END\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2775a54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langgraph.graph import END\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"llm\", llm_node)\n",
    "builder.add_node(\"tools\", tool_node)\n",
    "builder.add_node(\"reflect\", reflection_node)\n",
    "\n",
    "builder.add_edge(START, \"llm\")\n",
    "builder.add_conditional_edges(\"llm\", tools_condition, default_edge=\"reflect\")\n",
    "builder.add_edge(\"tools\", \"llm\")\n",
    "builder.add_edge(\"reflect\", END)\n",
    "\n",
    "react_reflect_graph = builder.compile()\n",
    "react_reflect_graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8cecaf",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Run an Example Query\n",
    "\n",
    "We’ll ask something that *requires* combining MCP, LangGraph, and ReAct:\n",
    "\n",
    "> *“Explain how MCP, LangGraph, and ReAct work together to build a powerful agentic AI system.”*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c3342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pprint import pprint\n",
    "\n",
    "question = (\n",
    "    \"Explain how MCP, LangGraph, and ReAct can work together to build a powerful agentic AI system. \"\n",
    "    \"Use the available lookup tool if needed.\"\n",
    ")\n",
    "\n",
    "state = {\"messages\": [{\"role\": \"user\", \"content\": question}]}\n",
    "result = react_reflect_graph.invoke(state)\n",
    "\n",
    "print(\"=== Final messages ===\")\n",
    "for i, m in enumerate(result[\"messages\"]):\n",
    "    print(f\"\\n[{i}] role={m['role']}\")\n",
    "    pprint(m[\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa635c63",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Takeaways\n",
    "\n",
    "- **ReAct** provides a **thinking pattern**: alternate between thoughts and actions.  \n",
    "- **LangGraph** gives a **programmable control structure** around that pattern.  \n",
    "- **Reflection** layers **self-critique** on top, improving reliability and depth.\n",
    "\n",
    "You can now adapt this graph to:\n",
    "- Add more tools (RAG, APIs, databases)  \n",
    "- Add stronger reflection (compare multiple candidate answers)  \n",
    "- Add memory for cross-session learning  \n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
